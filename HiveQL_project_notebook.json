{"paragraphs":[{"text":"# Hive ETL project","user":"anonymous","dateUpdated":"2019-09-25T17:30:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Hive ETL project</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1568561853427_-462151823","id":"20190915-153733_1645749039","dateCreated":"2019-09-15T15:37:33+0000","dateStarted":"2019-09-25T17:30:43+0000","dateFinished":"2019-09-25T17:30:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:99"},{"text":"## Hive CLI Clients\n\nThe two hive clients we have looked at are beeline and the native hive client. Hive and beeline differ mainly on the connection to the Hive. Beeline is a thin client that connects to HiveServer2 and does not require the installation of Hive libraries on the same machine as the client. Hive native on the other hand connects directly to the Hive driver and executes actions through there. Beeline also executes actions through the driver but uses HiveServer2 to do so. For the rest of this project I will use the native hive CLI client.","user":"anonymous","dateUpdated":"2019-09-25T17:30:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Hive CLI Clients</h2>\n<p>The two hive clients we have looked at are beeline and the native hive client. Hive and beeline differ mainly on the connection to the Hive. Beeline is a thin client that connects to HiveServer2 and does not require the installation of Hive libraries on the same machine as the client. Hive native on the other hand connects directly to the Hive driver and executes actions through there. Beeline also executes actions through the driver but uses HiveServer2 to do so. For the rest of this project I will use the native hive CLI client.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1568562794359_-436402335","id":"20190915-155314_104693964","dateCreated":"2019-09-15T15:53:14+0000","dateStarted":"2019-09-25T17:30:43+0000","dateFinished":"2019-09-25T17:30:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:100"},{"text":"## Creating a Hive Table from a Dataset\n\nThe dataset that will be used is the 2016 World Development Indicator (WDI) which was obtained from Google. The link to this data set is: <https://bigquery.cloud.google.com/table/patents-public-data:worldbank_wdi.wdi_2016>\n\nAfter the data was imported into my own Google storage bucket, the first step to working with this data is creating a table within Hive. The following queries were used to first create the table from the input file and then confirm the table was created by doing a count of the rows.\n\n```\nDROP TABLE IF EXISTS wdi_gs;\nCREATE EXTERNAL TABLE wdi_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nLOCATION 'gs://jrvs-bootcamp-andresim/datasets/wdi_2016'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n```\n\n```\nSELECT count(*) FROM wdi_gs;\n# Takes 33.82 seconds\n```","user":"anonymous","dateUpdated":"2019-09-25T17:30:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Creating a Hive Table from a Dataset</h2>\n<p>The dataset that will be used is the 2016 World Development Indicator (WDI) which was obtained from Google. The link to this data set is: <a href=\"https://bigquery.cloud.google.com/table/patents-public-data:worldbank_wdi.wdi_2016\">https://bigquery.cloud.google.com/table/patents-public-data:worldbank_wdi.wdi_2016</a></p>\n<p>After the data was imported into my own Google storage bucket, the first step to working with this data is creating a table within Hive. The following queries were used to first create the table from the input file and then confirm the table was created by doing a count of the rows.</p>\n<pre><code>DROP TABLE IF EXISTS wdi_gs;\nCREATE EXTERNAL TABLE wdi_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LINES TERMINATED BY &#39;\\n&#39;\nLOCATION &#39;gs://jrvs-bootcamp-andresim/datasets/wdi_2016&#39;\nTBLPROPERTIES (&quot;skip.header.line.count&quot;=&quot;1&quot;);\n</code></pre>\n<pre><code>SELECT count(*) FROM wdi_gs;\n# Takes 33.82 seconds\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1568562099269_526568492","id":"20190915-154139_162843737","dateCreated":"2019-09-15T15:41:39+0000","dateStarted":"2019-09-25T17:30:44+0000","dateFinished":"2019-09-25T17:30:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:101"},{"text":"## Loading Data into the Hive External Table\n\nQuerying the Google storage itself is inefficient, so in this next section a new Hive external table is created with it's own HDFS location. This table is then loaded with data from the Google storage. After connection to the Hive CLI client, the following steps and queries are run:\n\n1. Creating an external Hive table.\n```\nhdfs dfs -mkdir -p hdfs:///user/andresim/hive/wdi\n```\n2. Create the new table and run the INSERT query that will load data from wdi_gs to the new table.\n```\nDROP TABLE IF EXISTS wdi_csv_text;\nCREATE EXTERNAL TABLE wdi_csv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nLOCATION 'hdfs:///user/andresim/hive/wdi/wdi_csv_text';\n```\n```\nINSERT OVERWRITE TABLE wdi_csv_text\nSELECT * FROM wdi_gs;\n# Takes 84.83 seconds\n```\n3. Confirm the new file size after table overwrite is complete.\n```\nhdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_csv_text\n> 1.7 G  hdfs:///user/andresim/hive/wdi/wdi_csv_text\n```\n4. Run a count of the countries within the table multiple times to show the effect that caching has on read speeds. Running this query a second time has it complete is a shorter time. Clearing the cache and re-running again will have the next query run in a time similar to the first time it is run.\n```\nSELECT count(countryName) FROM wdi_csv_text;\n# Takes 28.88 seconds\n```\nbash command to clear caches in worker nodes (if we want to clear caches between runs for performance comparison):\n```\necho 3 | sudo tee /proc/sys/vm/drop_caches\n```\n\nTo get more information about the table I used the 'DESCRIBE FORMATTED wdi_csv_text' command in Hive to get further information. Below is an example of some sample information output to the terminal relating to the storage.\n```\n# Storage Information            \nSerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       \nInputFormat:            org.apache.hadoop.mapred.TextInputFormat         \nOutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n```","user":"anonymous","dateUpdated":"2019-09-25T17:30:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Loading Data into the Hive External Table</h2>\n<p>Querying the Google storage itself is inefficient, so in this next section a new Hive external table is created with it&rsquo;s own HDFS location. This table is then loaded with data from the Google storage. After connection to the Hive CLI client, the following steps and queries are run:</p>\n<ol>\n  <li>\n  <p>Creating an external Hive table.</p>\n  <pre><code>hdfs dfs -mkdir -p hdfs:///user/andresim/hive/wdi\n</code></pre></li>\n  <li>\n  <p>Create the new table and run the INSERT query that will load data from wdi_gs to the new table.</p>\n  <pre><code>DROP TABLE IF EXISTS wdi_csv_text;\nCREATE EXTERNAL TABLE wdi_csv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LINES TERMINATED BY &#39;\\n&#39;\nLOCATION &#39;hdfs:///user/andresim/hive/wdi/wdi_csv_text&#39;;\n</code></pre>\n  <pre><code>INSERT OVERWRITE TABLE wdi_csv_text\nSELECT * FROM wdi_gs;\n# Takes 84.83 seconds\n</code></pre></li>\n  <li>\n  <p>Confirm the new file size after table overwrite is complete.</p>\n  <pre><code>hdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_csv_text\n&gt; 1.7 G  hdfs:///user/andresim/hive/wdi/wdi_csv_text\n</code></pre></li>\n  <li>\n  <p>Run a count of the countries within the table multiple times to show the effect that caching has on read speeds. Running this query a second time has it complete is a shorter time. Clearing the cache and re-running again will have the next query run in a time similar to the first time it is run.</p>\n  <pre><code>SELECT count(countryName) FROM wdi_csv_text;\n# Takes 28.88 seconds\n</code></pre>\n  <p>bash command to clear caches in worker nodes (if we want to clear caches between runs for performance comparison):</p>\n  <pre><code>echo 3 | sudo tee /proc/sys/vm/drop_caches\n</code></pre></li>\n</ol>\n<p>To get more information about the table I used the &lsquo;DESCRIBE FORMATTED wdi_csv_text&rsquo; command in Hive to get further information. Below is an example of some sample information output to the terminal relating to the storage.</p>\n<pre><code># Storage Information            \nSerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       \nInputFormat:            org.apache.hadoop.mapred.TextInputFormat         \nOutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1568819922776_-1086999192","id":"20190918-151842_53019437","dateCreated":"2019-09-18T15:18:42+0000","dateStarted":"2019-09-25T17:30:44+0000","dateFinished":"2019-09-25T17:30:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"text":"## Comparing Hive and Bash\n\nTo compare Hive and Bash read times we will copy the wdi_csv_text directory to a local instance and then count the number of rows in all files in the directory. The cache will be cleared and then the bash time will be compared to the Hive time.\n\n1. Copy the wdi table to local and read the disk usage\n```\nhdfs dfs -get hdfs:///user/andresim/hive/wdi/wdi_csv_text\n```\n```\ncd wdi_csv_text\n```\n```\ndu -ch .\n> 1.8G total\n```\n2. Clear the cache\n```\necho 3 | sudo tee /proc/sys/vm/drop_caches\n```\n3. Count the rows using a bash command\n```\ndate +%s && cat * | wc && date +%s\n# Takes 32.47 seconds\n```\n\nIt is interesting to note that although Hive should be much faster than bash in theory, we did not see this reflected in the performance times. This is likely due to the connectivity slowness between the GCP VM where our Hive is being run and our local machine. If Hive wereto be setup locally the execution times would likely be faster.\n\nThe next thing to do is to increase the data size to see if we can get a bigger difference between Hive and bash despite the connectivity. The next steps will first create the bigger file by copying data from the intial wdi_csv_text dirctory, then it will put it into a Hive external table once again. With that external table we will compare the hive read time and then export it locally and compare the bash read time.\n\n4. Copy the wdi_csv_text to a new directory\n```\nhdfs dfs -cp hdfs:///user/andresim/hive/wdi/wdi_csv_text\nhdfs:///user/andresim/hive/wdi/wdi_csv_text_big\n```\n5. Create a larger file by copying parts of the old file (short bash script)\n```\nfor i in {1...20}\ndo\n    echo \"Copying round: $i\"\n    hdfs dfs -cp hdfs:///user/andresim/hive/wdi/wdi_csv_text_big/00000_0\nhdfs:///user/andresim/hive/wdi/wdi_csv_text_big/00000_0-$i\ndone\n```\n6. Confirm the new directory size\n```\nhdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_csv_text_big\n> 7.2 GB is new directory size\n```\n7. Create a Hive table for the new large file\n```\nDROP TABLE IF EXISTS wdi_csv_text_big;\nCREATE EXTERNAL TABLE wdi_csv_text_big\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nLOCATION 'hdfs:///user/andresim/hive/wdi/wdi_csv_text_big';\n```\n8. Run counts in Hive and bash and compare the times\n```\n#clear the cache to start\necho 3 | sudo tee /proc/sys/vm/drop_caches\n```\n```\nHive count:\nSELECT count(countryName) FROM wdi_csv_text_big;\n# Takes 73.38 seconds (no cache)\n```\n```\nBash count:\nhdfs dfs -get hdfs:///user/andresim/hive/wdi/wdi_csv_text_big\ncd wdi_csv_text_big\necho 3 | sudo tee /proc/sys/vm/drop_caches\ndate +%s && cat * | wc && date +%s\n# Takes 158.23 seconds (no cache)\n```\n\nIn this instance the read time difference between bash and Hive was more in-line with what we expected, as bash took almost double the time to read the larger table.","user":"anonymous","dateUpdated":"2019-09-25T17:30:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Comparing Hive and Bash</h2>\n<p>To compare Hive and Bash read times we will copy the wdi_csv_text directory to a local instance and then count the number of rows in all files in the directory. The cache will be cleared and then the bash time will be compared to the Hive time.</p>\n<ol>\n  <li>\n  <p>Copy the wdi table to local and read the disk usage</p>\n  <pre><code>hdfs dfs -get hdfs:///user/andresim/hive/wdi/wdi_csv_text\n</code></pre>\n  <pre><code>cd wdi_csv_text\n</code></pre>\n  <pre><code>du -ch .\n&gt; 1.8G total\n</code></pre></li>\n  <li>\n  <p>Clear the cache</p>\n  <pre><code>echo 3 | sudo tee /proc/sys/vm/drop_caches\n</code></pre></li>\n  <li>\n  <p>Count the rows using a bash command</p>\n  <pre><code>date +%s &amp;&amp; cat * | wc &amp;&amp; date +%s\n# Takes 32.47 seconds\n</code></pre></li>\n</ol>\n<p>It is interesting to note that although Hive should be much faster than bash in theory, we did not see this reflected in the performance times. This is likely due to the connectivity slowness between the GCP VM where our Hive is being run and our local machine. If Hive wereto be setup locally the execution times would likely be faster.</p>\n<p>The next thing to do is to increase the data size to see if we can get a bigger difference between Hive and bash despite the connectivity. The next steps will first create the bigger file by copying data from the intial wdi_csv_text dirctory, then it will put it into a Hive external table once again. With that external table we will compare the hive read time and then export it locally and compare the bash read time.</p>\n<ol>\n  <li>\n  <p>Copy the wdi_csv_text to a new directory</p>\n  <pre><code>hdfs dfs -cp hdfs:///user/andresim/hive/wdi/wdi_csv_text\nhdfs:///user/andresim/hive/wdi/wdi_csv_text_big\n</code></pre></li>\n  <li>\n  <p>Create a larger file by copying parts of the old file (short bash script)</p>\n  <pre><code>for i in {1...20}\ndo\necho &quot;Copying round: $i&quot;\nhdfs dfs -cp hdfs:///user/andresim/hive/wdi/wdi_csv_text_big/00000_0\nhdfs:///user/andresim/hive/wdi/wdi_csv_text_big/00000_0-$i\ndone\n</code></pre></li>\n  <li>\n  <p>Confirm the new directory size</p>\n  <pre><code>hdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_csv_text_big\n&gt; 7.2 GB is new directory size\n</code></pre></li>\n  <li>\n  <p>Create a Hive table for the new large file</p>\n  <pre><code>DROP TABLE IF EXISTS wdi_csv_text_big;\nCREATE EXTERNAL TABLE wdi_csv_text_big\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LINES TERMINATED BY &#39;\\n&#39;\nLOCATION &#39;hdfs:///user/andresim/hive/wdi/wdi_csv_text_big&#39;;\n</code></pre></li>\n  <li>\n  <p>Run counts in Hive and bash and compare the times</p>\n  <pre><code>#clear the cache to start\necho 3 | sudo tee /proc/sys/vm/drop_caches\n</code></pre>\n  <pre><code>Hive count:\nSELECT count(countryName) FROM wdi_csv_text_big;\n# Takes 73.38 seconds (no cache)\n</code></pre>\n  <pre><code>Bash count:\nhdfs dfs -get hdfs:///user/andresim/hive/wdi/wdi_csv_text_big\ncd wdi_csv_text_big\necho 3 | sudo tee /proc/sys/vm/drop_caches\ndate +%s &amp;&amp; cat * | wc &amp;&amp; date +%s\n# Takes 158.23 seconds (no cache)\n</code></pre></li>\n</ol>\n<p>In this instance the read time difference between bash and Hive was more in-line with what we expected, as bash took almost double the time to read the larger table.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1568820868486_576251907","id":"20190918-153428_2146442666","dateCreated":"2019-09-18T15:34:28+0000","dateStarted":"2019-09-25T17:30:44+0000","dateFinished":"2019-09-25T17:30:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"## Parsing Issues\n\nAs with all data, there appears to be some issues with data cleaning in our current dataset. If we run the following query:\n\n```\nSELECT distinct(indicatorCode)\nFROM wdi_csv_text\norder by indicatorCode\nlimit 20;\n```\n\nWe see some odd entries such as:\n```\n% export of goods\n%)\"\n(% of urban population)\"\n15+\nAtlas method (current US$)\"\n```\nNone of which are indicator codes.\n\nThis issue likely comes from some data with an unexpected format, perhaps with the quotes used since it seems a field may have been missed by the parser, but also maybe due to the presence of unexpected row field delimiter characters. In the next section a debug table will be created to help figure out the root of the problem.\n","user":"anonymous","dateUpdated":"2019-09-25T17:30:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Parsing Issues</h2>\n<p>As with all data, there appears to be some issues with data cleaning in our current dataset. If we run the following query:</p>\n<pre><code>SELECT distinct(indicatorCode)\nFROM wdi_csv_text\norder by indicatorCode\nlimit 20;\n</code></pre>\n<p>We see some odd entries such as:</p>\n<pre><code>% export of goods\n%)&quot;\n(% of urban population)&quot;\n15+\nAtlas method (current US$)&quot;\n</code></pre>\n<p>None of which are indicator codes.</p>\n<p>This issue likely comes from some data with an unexpected format, perhaps with the quotes used since it seems a field may have been missed by the parser, but also maybe due to the presence of unexpected row field delimiter characters. In the next section a debug table will be created to help figure out the root of the problem.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1568821671493_237362802","id":"20190918-154751_1102727282","dateCreated":"2019-09-18T15:47:51+0000","dateStarted":"2019-09-25T17:30:45+0000","dateFinished":"2019-09-25T17:30:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"## Creating a Debug Table\n\nWe want to create the debug table to display lines where the indicatorCode is not being recognized properly. We will first create a debug table that will just contain Strings. Then we will populate it with lines that have the same text pattern as thew few lines from above. The queries are as follows:\n\n```\n1. \nDROP TABLE IF EXISTS wdi_gs_debug;\nCREATE EXTERNAL TABLE wdi_gs_debug\n(line STRING)\nLOCATION 'gs://jrvs-bootcamp-andresim/datasets/wdi_2016';\n\nSELECT * FROM wdi_gs_debug \nLIMIT 10;\n```\n\n```\n2.\nSELECT DISTINCT(line)\nFROM wdi_gs_debug\nWHERE line LIKE \"%(\\% of urban population)\\\"%\"\nLIMIT 10;\n```\n\nThe following output is produced after 34.83 seconds:\n```\n1960,\"Bahamas, The\",BHS,\"Access to electricity, urban (% of urban population)\",EG.ELC.ACCS.UR.ZS,0\n```\n\nFrom this line we can see that the countryCode (BHS) is not being recognized as a string due to the lack of double quotes. This seems to throw the parser off and have it read in part of the indicatorName (Access to electricity, urban (% of urban population)) as the indicatorCode. Correcting this specific issue should be as easy as surrounding BHS with double quotes.","user":"anonymous","dateUpdated":"2019-09-25T17:30:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Creating a Debug Table</h2>\n<p>We want to create the debug table to display lines where the indicatorCode is not being recognized properly. We will first create a debug table that will just contain Strings. Then we will populate it with lines that have the same text pattern as thew few lines from above. The queries are as follows:</p>\n<pre><code>1. \nDROP TABLE IF EXISTS wdi_gs_debug;\nCREATE EXTERNAL TABLE wdi_gs_debug\n(line STRING)\nLOCATION &#39;gs://jrvs-bootcamp-andresim/datasets/wdi_2016&#39;;\n\nSELECT * FROM wdi_gs_debug \nLIMIT 10;\n</code></pre>\n<pre><code>2.\nSELECT DISTINCT(line)\nFROM wdi_gs_debug\nWHERE line LIKE &quot;%(\\% of urban population)\\&quot;%&quot;\nLIMIT 10;\n</code></pre>\n<p>The following output is produced after 34.83 seconds:</p>\n<pre><code>1960,&quot;Bahamas, The&quot;,BHS,&quot;Access to electricity, urban (% of urban population)&quot;,EG.ELC.ACCS.UR.ZS,0\n</code></pre>\n<p>From this line we can see that the countryCode (BHS) is not being recognized as a string due to the lack of double quotes. This seems to throw the parser off and have it read in part of the indicatorName (Access to electricity, urban (% of urban population)) as the indicatorCode. Correcting this specific issue should be as easy as surrounding BHS with double quotes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1568821992815_1391364440","id":"20190918-155312_568084140","dateCreated":"2019-09-18T15:53:12+0000","dateStarted":"2019-09-25T17:30:45+0000","dateFinished":"2019-09-25T17:30:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"text":"## Create a Table with OpenCSV SerDe\n\nIn this section a new Hive table is created using OpenCSVSerde on the Google storage data. This new table is then exported to a HDFS location. First the Google storage table is created with OpenCSVSerde, then the HDFS table is created to load the data into. The data is then loaded from the Google storage table into the HDFS table and verified with a basic SELECT query. Finally the run time between the newly created opencsv table and the previous csv table are compared.\n\n1. Create Google storage table with OpenCSVSerde and verify creation\n```\nDROP TABLE IF EXISTS wdi_opencsv_gs;\nCREATE EXTERNAL TABLE wdi_opencsv_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n    \"separatorChar\" = \",\",\n    \"quoteChar\" = \"\\\"\",\n    \"escapeChar\" = \"\\\\\")\nLOCATION 'gs://jrvs-bootcamp-andresim/datasets/wdi_2016';\n```\n```\nSELECT DISTINCT(indicatorCode)\nFROM wdi_opencsv_gs\nORDER BY indicatorCode\nLIMIT 20;\n# Takes 80.82 seconds\n```\n2. Create the HDFS table to overwrite with data\n```\nDROP TABLE IF EXISTS wdi_opencsv_text;\nCREATE EXTERNAL TABLE wdi_opencsv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nLOCATION 'hdfs:///user/andresim/hive/wdi/wdi_opencsv_text';\n```\n3. Overwrite the HDFS table with data from Google storage\n```\nINSERT OVERWRITE TABLE wdi_opencsv_text\nSELECT * FROM wdi_opencsv_gs;\n# Takes 117.48 seconds.\n```\n4. Run basic query to verify the table was created properly\n```\nSELECT DISTINCT(indicatorCode)\nFROM wdi_opencsv_text\nORDER BY indicatorCode\nLIMIT 20;\n# Takes 76.79 seconds\n```\n5. Compare run times between original csv table and new opencsv table\n```\nSELECT count(countryName) FROM wdi_opencsv_text;\n# Takes 73.74 seconds\nSELECT count(countryName) FROM wdi_csv_text;\n# Takes 24.70 seconds\n```\n### OpenCSV SerDe Limitations and Properties\n\nThe OpenCSV SerDe used does have a few unique properties and limitations. It:\n\n- Converts all column type values to STRING.\n\n- Relies on a parser to deal with non-string values and converts the values from STRING into those data types if it can recognize them.\n\n- Uses double quotes (\") as the default quote character, and allows you to specify separator, quote, and escape characters.\n\n- Cannot escape \\t or \\n directly. To escape them, use \"escapeChar\" = \"\\\\\". This can cause confusion and issues depending on the data entered.\n\n- Does not support embedded line breaks in CSV files.","user":"anonymous","dateUpdated":"2019-09-25T17:30:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Create a Table with OpenCSV SerDe</h2>\n<p>In this section a new Hive table is created using OpenCSVSerde on the Google storage data. This new table is then exported to a HDFS location. First the Google storage table is created with OpenCSVSerde, then the HDFS table is created to load the data into. The data is then loaded from the Google storage table into the HDFS table and verified with a basic SELECT query. Finally the run time between the newly created opencsv table and the previous csv table are compared.</p>\n<ol>\n  <li>\n  <p>Create Google storage table with OpenCSVSerde and verify creation</p>\n  <pre><code>DROP TABLE IF EXISTS wdi_opencsv_gs;\nCREATE EXTERNAL TABLE wdi_opencsv_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;\nWITH SERDEPROPERTIES (\n&quot;separatorChar&quot; = &quot;,&quot;,\n&quot;quoteChar&quot; = &quot;\\&quot;&quot;,\n&quot;escapeChar&quot; = &quot;\\\\&quot;)\nLOCATION &#39;gs://jrvs-bootcamp-andresim/datasets/wdi_2016&#39;;\n</code></pre>\n  <pre><code>SELECT DISTINCT(indicatorCode)\nFROM wdi_opencsv_gs\nORDER BY indicatorCode\nLIMIT 20;\n# Takes 80.82 seconds\n</code></pre></li>\n  <li>\n  <p>Create the HDFS table to overwrite with data</p>\n  <pre><code>DROP TABLE IF EXISTS wdi_opencsv_text;\nCREATE EXTERNAL TABLE wdi_opencsv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;\nLOCATION &#39;hdfs:///user/andresim/hive/wdi/wdi_opencsv_text&#39;;\n</code></pre></li>\n  <li>\n  <p>Overwrite the HDFS table with data from Google storage</p>\n  <pre><code>INSERT OVERWRITE TABLE wdi_opencsv_text\nSELECT * FROM wdi_opencsv_gs;\n# Takes 117.48 seconds.\n</code></pre></li>\n  <li>\n  <p>Run basic query to verify the table was created properly</p>\n  <pre><code>SELECT DISTINCT(indicatorCode)\nFROM wdi_opencsv_text\nORDER BY indicatorCode\nLIMIT 20;\n# Takes 76.79 seconds\n</code></pre></li>\n  <li>\n  <p>Compare run times between original csv table and new opencsv table</p>\n  <pre><code>SELECT count(countryName) FROM wdi_opencsv_text;\n# Takes 73.74 seconds\nSELECT count(countryName) FROM wdi_csv_text;\n# Takes 24.70 seconds\n</code></pre>\n  <h3>OpenCSV SerDe Limitations and Properties</h3></li>\n</ol>\n<p>The OpenCSV SerDe used does have a few unique properties and limitations. It:</p>\n<ul>\n  <li>\n  <p>Converts all column type values to STRING.</p></li>\n  <li>\n  <p>Relies on a parser to deal with non-string values and converts the values from STRING into those data types if it can recognize them.</p></li>\n  <li>\n  <p>Uses double quotes (&quot;) as the default quote character, and allows you to specify separator, quote, and escape characters.</p></li>\n  <li>\n  <p>Cannot escape \\t or \\n directly. To escape them, use &ldquo;escapeChar&rdquo; = &ldquo;\\&rdquo;. This can cause confusion and issues depending on the data entered.</p></li>\n  <li>\n  <p>Does not support embedded line breaks in CSV files.</p></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1568822562838_1339420959","id":"20190918-160242_1071497638","dateCreated":"2019-09-18T16:02:42+0000","dateStarted":"2019-09-25T17:30:45+0000","dateFinished":"2019-09-25T17:30:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"text":"## 2015 Canada GDP Growth Hive Query\n\nThis section will focus on finding the 2015 GDP growth for Canada. This will be done in two queries. The first will find the correct indicator code and the second will use this indicator code to get the information for Canada in the year 2015.\n\n```\n1.\nSELECT DISTINCT (indicatorCode), indicatorname\nFROM wdi_opencsv_text\nWHERE indicatorCode LIKE '%GDP%';\n```\n\nThis gives us the indicator code \"NY.GDP.MKTP.KD.ZG\" which is the indicator code for annual GDP % growth.\n\n```\n2.\nSELECT indicatorValue, year, countryName\nFROM wdi_opencsv_text\nWHERE indicatorCode = 'NY.GDP.MKTP.KD.ZG'\nAND year = '2015' \nAND countryName = 'Canada';\n# Takes 76.0 seconds\n```\n\nAn important point to note is that the second query takes quite long. This is due to the table size in combination with the additional restrictions in the WHERE clause. Hive will need to first search for all indicator codes, then from the set of like indicator codes it will need to search for those with year = 2015 before finally searching the subset for entries which have country = Canada. In practice we can make this more efficient with paritioning on the Hive table which will be done in the next section.","user":"anonymous","dateUpdated":"2019-09-25T17:30:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>2015 Canada GDP Growth Hive Query</h2>\n<p>This section will focus on finding the 2015 GDP growth for Canada. This will be done in two queries. The first will find the correct indicator code and the second will use this indicator code to get the information for Canada in the year 2015.</p>\n<pre><code>1.\nSELECT DISTINCT (indicatorCode), indicatorname\nFROM wdi_opencsv_text\nWHERE indicatorCode LIKE &#39;%GDP%&#39;;\n</code></pre>\n<p>This gives us the indicator code &ldquo;NY.GDP.MKTP.KD.ZG&rdquo; which is the indicator code for annual GDP % growth.</p>\n<pre><code>2.\nSELECT indicatorValue, year, countryName\nFROM wdi_opencsv_text\nWHERE indicatorCode = &#39;NY.GDP.MKTP.KD.ZG&#39;\nAND year = &#39;2015&#39; \nAND countryName = &#39;Canada&#39;;\n# Takes 76.0 seconds\n</code></pre>\n<p>An important point to note is that the second query takes quite long. This is due to the table size in combination with the additional restrictions in the WHERE clause. Hive will need to first search for all indicator codes, then from the set of like indicator codes it will need to search for those with year = 2015 before finally searching the subset for entries which have country = Canada. In practice we can make this more efficient with paritioning on the Hive table which will be done in the next section.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1568822591482_-1801793220","id":"20190918-160311_1085436669","dateCreated":"2019-09-18T16:03:11+0000","dateStarted":"2019-09-25T17:30:45+0000","dateFinished":"2019-09-25T17:30:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"text":"## Hive Partitions\n\nThis section will attempt to optimize the GDP growth query from above using partitions. First, a table will be created to store the data partitioned by year. Then the table will be overwritten with data partitioned on year. After, a command will be run to inspect the partitions created before finally running the same query from the previous section on the partitions.\n\n1. Create the table\n```\nDROP TABLE IF EXISTS wdi_opencsv_text_partitions;\nCREATE EXTERNAL TABLE wdi_opencsv_text_partitions\n(countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT) PARTITIONED BY(year INTEGER)\nROW FORMAT DELIMITED\nLOCATION 'hdfs:///user/andresim/hive/wdi/wdi_opencsv_text_partitions';\n```\n2. Overwrite the table with partitioned data\n```\nset hive.exec.dynamic.partition.mode=nonstrict;\nINSERT OVERWRITE TABLE wdi_opencsv_text_partitions PARTITION (year)\nSELECT countryName, countryCode, indicatorName, indicatorCode, indicatorValue, YEAR\nFROM wdi_opencsv_text;\n# Takes 133.56 seconds\n```\n3. Inspect partitions created\n```\nhdfs dfs -ls hdfs:///user/andresim/hive/wdi/wdi_opencsv_text_partitions\n```\n4. Run Canadian GDP Growth query from previous section\n```\nSELECT indicatorValue, year, countryName\nFROM wdi_opencsv_text_partitions\nWHERE indicatorCode = 'NY.GDP.MKTP.KD.ZG'\nAND year = '2015' \nAND countryName = 'Canada';\n# Takes 4.14 seconds (MUCH faster than without partitions)\n```\n\nThis previous query will only read the 2015 partition file which has a size of 35980940 bytes or 35.98 MB, which is one reason why it runs so much quicker.","user":"anonymous","dateUpdated":"2019-09-25T17:30:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Hive Partitions</h2>\n<p>This section will attempt to optimize the GDP growth query from above using partitions. First, a table will be created to store the data partitioned by year. Then the table will be overwritten with data partitioned on year. After, a command will be run to inspect the partitions created before finally running the same query from the previous section on the partitions.</p>\n<ol>\n  <li>\n  <p>Create the table</p>\n  <pre><code>DROP TABLE IF EXISTS wdi_opencsv_text_partitions;\nCREATE EXTERNAL TABLE wdi_opencsv_text_partitions\n(countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT) PARTITIONED BY(year INTEGER)\nROW FORMAT DELIMITED\nLOCATION &#39;hdfs:///user/andresim/hive/wdi/wdi_opencsv_text_partitions&#39;;\n</code></pre></li>\n  <li>\n  <p>Overwrite the table with partitioned data</p>\n  <pre><code>set hive.exec.dynamic.partition.mode=nonstrict;\nINSERT OVERWRITE TABLE wdi_opencsv_text_partitions PARTITION (year)\nSELECT countryName, countryCode, indicatorName, indicatorCode, indicatorValue, YEAR\nFROM wdi_opencsv_text;\n# Takes 133.56 seconds\n</code></pre></li>\n  <li>\n  <p>Inspect partitions created</p>\n  <pre><code>hdfs dfs -ls hdfs:///user/andresim/hive/wdi/wdi_opencsv_text_partitions\n</code></pre></li>\n  <li>\n  <p>Run Canadian GDP Growth query from previous section</p>\n  <pre><code>SELECT indicatorValue, year, countryName\nFROM wdi_opencsv_text_partitions\nWHERE indicatorCode = &#39;NY.GDP.MKTP.KD.ZG&#39;\nAND year = &#39;2015&#39; \nAND countryName = &#39;Canada&#39;;\n# Takes 4.14 seconds (MUCH faster than without partitions)\n</code></pre></li>\n</ol>\n<p>This previous query will only read the 2015 partition file which has a size of 35980940 bytes or 35.98 MB, which is one reason why it runs so much quicker.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1568822637330_-1658640666","id":"20190918-160357_106325496","dateCreated":"2019-09-18T16:03:57+0000","dateStarted":"2019-09-25T17:30:46+0000","dateFinished":"2019-09-25T17:30:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"## Further Optimization Through Columnar File\n\nPerformance of the Hive queries can be optimized further through use of a columnar file. Columnar files store column information as 'rows', similar if you were to transpose a matrix of the data. This helps mainly with file compression, as columns typically contain similar types of data and sometimes contain only a few unique values across an entire column, thus allowing for the column data to be compressed much more than the row data.\n\nIn this section a new hive external table which stores the data in parquet format is created, overwritten with data from a previous table and then tested for disk usage and read speed. Finally, the 2015 GDP growth query will be executed on both to compare the runtime on that as well. The queries and bash commands to do this are as follows:\n\n1. Create the parquet table\n```\nDROP TABLE IF EXISTS wdi_csv_parquet;\nCREATE EXTERNAL TABLE wdi_csv_parquet\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nSTORED AS PARQUET\nLOCATION 'hdfs:///user/andresim/hive/wdi/wdi_csv_parquet';\n```\n2. Populate the table with data \n```\nINSERT OVERWRITE TABLE wdi_csv_parquet\nSELECT * FROM wdi_opencsv_gs;\n# Takes 117.09 seconds\n```\n3. Compare file sizes\n```\nhdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_csv_parquet\n> 137.2 MB in size\nhdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_opencsv_text\n> 2.3 GB in size\n```\n4. Compare runtime of a simple select query\n```\nSELECT count(countryName) FROM wdi_csv_parquet;\n# Takes 24.88 seconds\nSELECT count(countryName) FROM wdi_opencsv_text;\n# Takes 70.53 seconds\n```\n5. Compare runtime of 2015 GDP growth query\n```\nSELECT indicatorValue, year, countryName\nFROM wdi_opencsv_text\nWHERE indicatorCode = 'NY.GDP.MKTP.KD.ZG'\nAND year = '2015';\n# Takes 70.38 seconds\n```\n```\nSELECT indicatorValue, year, countryName\nFROM wdi_csv_parquet\nWHERE indicatorCode = 'NY.GDP.MKTP.KD.ZG'\nAND year = '2015';\n# Takes 0.249 seconds\n```","user":"anonymous","dateUpdated":"2019-09-25T17:30:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Further Optimization Through Columnar File</h2>\n<p>Performance of the Hive queries can be optimized further through use of a columnar file. Columnar files store column information as &lsquo;rows&rsquo;, similar if you were to transpose a matrix of the data. This helps mainly with file compression, as columns typically contain similar types of data and sometimes contain only a few unique values across an entire column, thus allowing for the column data to be compressed much more than the row data.</p>\n<p>In this section a new hive external table which stores the data in parquet format is created, overwritten with data from a previous table and then tested for disk usage and read speed. Finally, the 2015 GDP growth query will be executed on both to compare the runtime on that as well. The queries and bash commands to do this are as follows:</p>\n<ol>\n  <li>\n  <p>Create the parquet table</p>\n  <pre><code>DROP TABLE IF EXISTS wdi_csv_parquet;\nCREATE EXTERNAL TABLE wdi_csv_parquet\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nSTORED AS PARQUET\nLOCATION &#39;hdfs:///user/andresim/hive/wdi/wdi_csv_parquet&#39;;\n</code></pre></li>\n  <li>\n  <p>Populate the table with data </p>\n  <pre><code>INSERT OVERWRITE TABLE wdi_csv_parquet\nSELECT * FROM wdi_opencsv_gs;\n# Takes 117.09 seconds\n</code></pre></li>\n  <li>\n  <p>Compare file sizes</p>\n  <pre><code>hdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_csv_parquet\n&gt; 137.2 MB in size\nhdfs dfs -du -s -h hdfs:///user/andresim/hive/wdi/wdi_opencsv_text\n&gt; 2.3 GB in size\n</code></pre></li>\n  <li>\n  <p>Compare runtime of a simple select query</p>\n  <pre><code>SELECT count(countryName) FROM wdi_csv_parquet;\n# Takes 24.88 seconds\nSELECT count(countryName) FROM wdi_opencsv_text;\n# Takes 70.53 seconds\n</code></pre></li>\n  <li>\n  <p>Compare runtime of 2015 GDP growth query</p>\n  <pre><code>SELECT indicatorValue, year, countryName\nFROM wdi_opencsv_text\nWHERE indicatorCode = &#39;NY.GDP.MKTP.KD.ZG&#39;\nAND year = &#39;2015&#39;;\n# Takes 70.38 seconds\n</code></pre>\n  <pre><code>SELECT indicatorValue, year, countryName\nFROM wdi_csv_parquet\nWHERE indicatorCode = &#39;NY.GDP.MKTP.KD.ZG&#39;\nAND year = &#39;2015&#39;;\n# Takes 0.249 seconds\n</code></pre></li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1568822648321_1420329010","id":"20190918-160408_1985648426","dateCreated":"2019-09-18T16:04:08+0000","dateStarted":"2019-09-25T17:30:46+0000","dateFinished":"2019-09-25T17:30:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"## Other Sample Hive Queries\nThis section contains a couple of sample queries that are a bit more complicated but use some of the unique HiveQL features.\n\n### Sorting GDP by Country and Year Hive Query\nThis query will return GDP growth for all countries and sort them by the countryName and the year.\n\n```\nSELECT countryName, year, indicatorCode, indicatorValue\nFROM wdi_csv_parquet\nWHERE indicatorCode = 'NY.GDP.MKTP.KD.ZG'\nDISTRIBUTE BY countryName\nSORT BY countryName, year\nLIMIT 200;\n# Takes 24.27 seconds\n```\n\n### Highest GDP Growth Hive Query\nThis query will find the highest GDP Growth year for each country. This query is a bit more complex and makes use of some HiveQL specific syntax.\n\n```\nSELECT wdi_csv_parquet.indicatorValue AS value, wdi_csv_parquet.year AS year, wdi_csv_parquet.countryName AS country\nFROM (SELECT Max(indicatorValue) as ind, countryName\n      FROM wdi_csv_parquet\n      WHERE indicatorCode = 'NY.GDP.MKTP.KD.ZG'\n      AND indicatorValue <> 0\n      GROUP BY countryName) t1\n      INNER JOIN wdi_csv_parquet\n          ON t1.ind = wdi_csv_parquet.indicatorValue\n          AND t1.countryName = wdi_csv_parquet.countryName;\n# Takes 103.62 seconds\n```\n\nSince the dataproc cluster also has spark-sql setup, the above query is also run with spark to compare performance. With spark-sql configured to have 2g of memory and 2 cores, the query runs in less than half the time without caching and about one seventh the time with caching.\n","user":"anonymous","dateUpdated":"2019-09-25T17:30:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Other Sample Hive Queries</h2>\n<p>This section contains a couple of sample queries that are a bit more complicated but use some of the unique HiveQL features.</p>\n<h3>Sorting GDP by Country and Year Hive Query</h3>\n<p>This query will return GDP growth for all countries and sort them by the countryName and the year.</p>\n<pre><code>SELECT countryName, year, indicatorCode, indicatorValue\nFROM wdi_csv_parquet\nWHERE indicatorCode = &#39;NY.GDP.MKTP.KD.ZG&#39;\nDISTRIBUTE BY countryName\nSORT BY countryName, year\nLIMIT 200;\n# Takes 24.27 seconds\n</code></pre>\n<h3>Highest GDP Growth Hive Query</h3>\n<p>This query will find the highest GDP Growth year for each country. This query is a bit more complex and makes use of some HiveQL specific syntax.</p>\n<pre><code>SELECT wdi_csv_parquet.indicatorValue AS value, wdi_csv_parquet.year AS year, wdi_csv_parquet.countryName AS country\nFROM (SELECT Max(indicatorValue) as ind, countryName\n      FROM wdi_csv_parquet\n      WHERE indicatorCode = &#39;NY.GDP.MKTP.KD.ZG&#39;\n      AND indicatorValue &lt;&gt; 0\n      GROUP BY countryName) t1\n      INNER JOIN wdi_csv_parquet\n          ON t1.ind = wdi_csv_parquet.indicatorValue\n          AND t1.countryName = wdi_csv_parquet.countryName;\n# Takes 103.62 seconds\n</code></pre>\n<p>Since the dataproc cluster also has spark-sql setup, the above query is also run with spark to compare performance. With spark-sql configured to have 2g of memory and 2 cores, the query runs in less than half the time without caching and about one seventh the time with caching.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1568822667630_1595855525","id":"20190918-160427_501439159","dateCreated":"2019-09-18T16:04:27+0000","dateStarted":"2019-09-25T17:30:58+0000","dateFinished":"2019-09-25T17:30:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:110"}],"name":"HiveQL_project_notebook","id":"2EQGW2TBS","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}